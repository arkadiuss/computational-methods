\documentclass{article}

\usepackage{hyperref}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}

\newcommand{\image}[2]{\begin{figure}[H]
	\centering	
	\includegraphics[width=\textwidth]{../images/#1}
	\caption{#2}
\end{figure}}

\begin{document}

\title{Sprawozdanie - Latent Semantic Indexing}
\author{Arkadiusz Kraus}

\maketitle


\section{Zadanie}
Zadanie polegało na zastosowaniu metody Latent Semantic Indexing (LSI), aby w zbiorze tektów móc wyszukiwać te związane z podaną frazą.

\section{Zbiór danych}
Zbiór danych jaki zastosowałem pochodzi ze strony \url{https://www.kaggle.com/snapcrack/all-the-news}. Jest to zbiór artykułów z amerykańskich gazet z lat 2016-2017 o różnej tematyce. Jest złożony z ponad 140 tys. tekstów.

\section{Zastosowane przekształcenia}
Na początku przetwarzamy wstępnie przetwarzamy zbiór artykułów, aby później móc skuteczniej w nim wyszukiwać:
\subsection{Zbiór użytych słów}
Dla każdego artykułu bierzemy wszyskie użyte słowa i tworzymy wektor słów dla wszystkich tekstów. Dla kolejnych podzbiorów artykułów posiada on następujące wielkości.
\begin{center}
	\begin{tabular}{c|c}
	liczba artykułów & rozmiar wektora \\ \hline
	1000 &  ~44 tys. \\ \hline
	10000 & ~140 tys.\\ \hline
	50000 & ~230 tys.\\ \hline
	wszystkie(142572) & ~400 tys.\\
	\end{tabular}
\end{center}
\subsection{Wstępne przetworzenie}
Liczba słów uzyskana w poprzednim podpunkcie jest bardzo duża, aby uczynić algorytm wydajniejszym staramy się zredukować ten wektor. We wstępnym przetworzeniu pomijamy wielkość liter oraz znaki interpunkcyjne.
\subsection{Stemming}
Wiele słów może występować w różnych formach np. take i taking. Nas jednak interesuje sam fakt czynności, a nie koniecznie np. czas w jakim była wykonywana. Innym przypadkiem jest też liczba mnoga rzeczownika, gdzie nadal interesuje nas sam rzeczownik. Dlatego stosujemy algorytm stemmingu (dokładniej Porter Stemmer), doprowadzamy słowa do wspólnego rdzenia i usuwamy duplikaty. 
W połączeniu z poprzednim krokiem pozwala to znacząco zredukowac rozmiar wektora:
\begin{center}
	\begin{tabular}{c|c}
	liczba artykułów & rozmiar wektora \\ \hline
	1000 &  ~24 tys.\\ \hline
	10000 & ~70 tys.\\ \hline
	50000 & ~127 tys. \\ \hline
	wszystkie(142572) & ~234tys.\\
	\end{tabular}
\end{center}
\subsection{Stopwords}
Stopwords to zbiór słów takich jak przyimki itp, które występują bardzo często we wszelkich tekstach. Nie mają one większego znaczenia dla treści dlatego również można je pominąć. Nie redukuje to wielkości wektora znacznie (o około 150 słów), jednak zwiększa jakość wyszukiwania.
\subsection{Tworzenie macierzy rzadkiej}
Chcemy teraz utworzyć macierz, która jako wiersze będzie przyjmować kolejne artukułu, a jako kolumny kolejne słowa ze zbioru. Wartością w komórce $a_ij$ macierzy będzie liczba wystąpień j-tego słowa w i-tym artykule. Zauważmy, że będzie to macierz rzadka ponieważ artyukuły mają ok. 4000 tys słów, a wektor słów jest znacznie większy. Dodatkowo jeśli artykuł jest na jakis temat to słowa często się w nim powtarzają.
Aby przyśpieszyć obliczenia (i w ogóle je umożliwić) stosujemy więc przechowywanie w postaci macierzy rzadkiej (dokładniej CSR - Compressed Sparse Row).
\subsection{IDF}
W celu zwiększenia jakości wyszukiwania stosujemy Inverse Document Frequency (IDF). Pozwala on nam zmniejszyć znaczenie słów, które występują w wielu artykułach i zwiększyć tych, które rzadko.
\subsection{Normalizacja}
Normalizacja pozwala uniezależnić korelację od długości tekstu. Wykonujemy ją od razu, aby potem przyspieszyć wyszukiwanie, aby nie trzeba było obliczać normy za każdym razem.
\subsection{Odszumianie}
W celu redukcji szumów stosujemy algorytm SVD i wybieramy k najciekawszych wartości własnych. Co ciekawe zwiększa to liczbę niezerowych elementów macierzy z ok 1-2 mln do 87 mln dla 10 tys. artykułów.

\section{Obliczenia}
Problem ten jest bardzo wymagający obliczeniowo oraz pamięciowo. Przechowanie macierzy wielkości \textit{liczba artykułów * liczba słów} dla większej ilości artukułów skutkuje skończeniem pamięci. W pliku \textit{logs} znajdują się czasy wstępnego przetworzenia dla 50000 i wszystkich artykułów. Wyszukiwanie po przetworzeniu i odczytaniu wcześniejszej macierzy jest również zależne od ilości artykułów i dla większej ilości jeszcze trochę mu brakuje względem czasów osiągnych przez Google.

\begin{center}
	\begin{tabular}{c|c|p{1.5cm}|p{1.6cm}|c}
	\centering liczba artykułów & rozmiar wektora & rozmiar wektora po przetworzeniu & czas przetworzenia & czas wyszukania \\ \hline
	1000 & ~44 tys & ~24 tys & natychmiast & natychmiast\\ \hline
	10000 & ~140 tys & ~70 tys & ~1 min & 1 s\\ \hline
	50000 & ~230 tys & 127 tys & 35 min & 15 sek\\ \hline
	wszystkie(142572) & ~400 tys & ~234 tys & 2,5h & 1 min\\
	\end{tabular}
\end{center}

Niestety dla 50000 oraz dla wszystkich artykułów stworzenie macierzy po dekompozycji SVD okazało się zbyt czasochłonne. Przy próbie utworzenia normalnej macierzy po dekompozycji dla np. 200 wartości własnych otrzymywałem brak pamięci, natomiast dla macierzy rzadkich obliczenia trwały w nieskończoność (obliczenia na nich są czasochłonnymi operacjami). Dlatego dla tych wielkości wyszukiwarka używa macierzy bez odszumienia.

\section{Wyniki}

Aplikacja jest podzielona na dwie części - jedna odpowiadająca za indeksowanie - articles\_processor.py i druga wyszukująca treści dla zadanej frazy  - search.py. Obie częsci działają poprawnie. Poniżej znajduje się przykład:

\image{weather_rain_south.png}{Wyniki wyszukiwania dla hasła "weather rain south"}

\image{weather_rain_south_2.png}{Wyniki wyszukiwania dla hasła "weather rain south" część druga}

Dla większej ilości artukułów gdzie nie było możliwe przeprowadzenie SVD zdarzają się jednak wyniki takie jak np. ten:

\image{baseball_fan_with_noise.png}{Przykład wyniku z szumami}

W tym przypadku dopiero któryś wynik jest taki jak powinien, a na początku jest szum. To samo zapytanie przy mniejszej ilości artykułów i z usuniętym szumem wygląda następująco:

\image{baseball_fan_without_noise.png}{Przykład wyniku bez szumu}

Przeanalizujmy jeszcze wyniki dla różnych współczynników k przy SVD:

Bez SVD:
\image{trumps_wife_no_svd.png}{Wynik dla zapytania "Trump's wife" bez svd}

Dla k = 50:
\image{trumps_wife_50.png}{Wynik dla zapytania "Trump's wife" i k = 50}
\image{trumps_wife_50_2.png}{Wynik dla zapytania "Trump's wife" i k = 50 część druga}

Dla k = 100:
\image{trumps_wife_100.png}{Wynik dla zapytania "Trump's wife" i k = 100}

Dla k = 200:
\image{trumps_wife_200.png}{Wynik dla zapytania "Trump's wife" i k = 200}

Dla k = 1000:
\image{trumps_wife_1000.png}{Wynik dla zapytania "Trump's wife" i k = 1000}

Widzimy, że najlepsze wyniki otrzymaliśmy dla k w okoliczach 100-200. Bez SVD występowały szumu w postaci artykułów o dużej korelacji jednak bardzo krótkich i mało spokrewnionych z tematem. Powróciły one również dla k = 1000. Dla głównych artykułów korelacja nie zmieniała się znacząco jednak zmienia się ona dla szumu.

\section{Wnioski}
Algorytm LSI dobrze sprawdził się przy wyszukiwaniu frazy w podanym zbiorze tekstów. Zajmuje trochę czasu jego wcześniejsze przetworzenie i indeksacja, ale nie są to bardzo duże czasy. Jednak myślę, że w tej wersji nie sprawdziłby się w silniku Googla. \\
Aby go polepszyć możnaby próbować zredukować wielkość wektora słów jeszcze bardziej ponieważ nadal po wypisaniu go widać szumy. Znaczący wpływ ma również SVD, które zostało przeze mnie pominięte dla większej ilości artykułów z powodów ograniczeń technicznych.\\
Ciekawą częścią algorytmu jest to, że każdy jego element można zapisać w cache'u - komentarze w pliku articles\_processor.py - co znacząco przyśpiesza pracę. 
\end{document}